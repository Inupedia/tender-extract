"""
LLMè·¯ç”±æ¨¡å—
"""
import json
import time
import re
from typing import List, Dict, Any, Optional, Tuple
from .schema import LLMRequest, LLMResponse, EvidenceSpan, ExtractedField
import logging

logger = logging.getLogger(__name__)

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    logger.warning("openaiæœªå®‰è£…ï¼ŒOpenAIåŠŸèƒ½å°†ä¸å¯ç”¨")

try:
    import ollama
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False
    logger.warning("ollamaæœªå®‰è£…ï¼ŒOllamaåŠŸèƒ½å°†ä¸å¯ç”¨")


class LLMRouter:
    """LLMè·¯ç”±å™¨"""
    
    def __init__(self, provider: str = "none", model: Optional[str] = None, 
                 api_key: Optional[str] = None, base_url: Optional[str] = None,
                 debug_mode: bool = False):
        self.provider = provider.lower()
        self.model = model
        self.api_key = api_key
        self.base_url = base_url
        self.debug_mode = debug_mode
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self.client = None
        self._initialize_client()
        
        # ç¼“å­˜
        self.cache = {}
        self.cache_hits = 0
        
        # ç»Ÿè®¡
        self.total_calls = 0
        self.successful_calls = 0
        self.failed_calls = 0
    
    def _initialize_client(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        if self.provider == "openai" and OPENAI_AVAILABLE:
            if self.api_key:
                openai.api_key = self.api_key
            if self.base_url:
                openai.api_base = self.base_url
            self.client = openai
        elif self.provider == "ollama" and OLLAMA_AVAILABLE:
            if self.base_url:
                self.client = ollama.Client(host=self.base_url)
            else:
                self.client = ollama.Client()
        else:
            logger.warning(f"LLMæä¾›å•† {self.provider} ä¸å¯ç”¨æˆ–æœªé…ç½®")
    
    def should_use_llm(self, field: ExtractedField, confidence_threshold: float = 0.7) -> bool:
        """æŒ‰éœ€LLMè·¯ç”±ç­–ç•¥ - ä»…å½“è§„åˆ™å±‚ä½ç½®ä¿¡æˆ–å†²çªæ—¶ä½¿ç”¨LLM"""
        if self.provider == "none" or not self.client:
            return False
        
        # 1. ç½®ä¿¡åº¦ä½äºé˜ˆå€¼ï¼ˆæ ¸å¿ƒç­–ç•¥ï¼‰
        if field.confidence < confidence_threshold:
            return True
        
        # 2. å­˜åœ¨å†²çªï¼ˆå¤šä¸ªä¸åŒå€¼ï¼‰
        if field.conflicts:
            return True
        
        # 3. å€¼è´¨é‡æ£€æŸ¥
        if field.values:
            primary_value = field.values[0].value
            # å€¼é•¿åº¦å¼‚å¸¸
            if len(primary_value) < 2 or len(primary_value) > 200:
                return True
            # åŒ…å«ç‰¹æ®Šå­—ç¬¦
            if re.search(r'[<>{}[\]()]', primary_value):
                return True
            # å…¨æ˜¯æ•°å­—æˆ–ç¬¦å·
            if re.match(r'^[\d\s\-_\.]+$', primary_value):
                return True
        
        # 4. é«˜ä¼˜å…ˆçº§å­—æ®µçš„ä¸¥æ ¼æ£€æŸ¥
        high_priority_fields = ['amount', 'date', 'number', 'deposit', 'contact', 'bid_amount']
        if field.field_type in high_priority_fields and field.confidence < 0.8:
            return True
        
        return False
    
    def get_minimal_evidence_context(self, field: ExtractedField, chunk_text: str) -> str:
        """è·å–æœ€å°è¯æ®ç‰‡æ®µï¼Œç”¨äºLLMå¤„ç†"""
        if not field.values:
            return chunk_text[:500]  # é»˜è®¤è¿”å›å‰500å­—ç¬¦
        
        # è·å–æ‰€æœ‰è¯æ®ç‰‡æ®µçš„ä½ç½®
        spans = []
        for evidence in field.values:
            spans.append((evidence.start, evidence.end))
        
        # åˆå¹¶é‡å çš„ç‰‡æ®µ
        merged_spans = self._merge_overlapping_spans(spans)
        
        # æå–æœ€å°ä¸Šä¸‹æ–‡
        contexts = []
        for start, end in merged_spans:
            # æ‰©å±•ä¸Šä¸‹æ–‡ï¼ˆå‰åå„100å­—ç¬¦ï¼‰
            context_start = max(0, start - 100)
            context_end = min(len(chunk_text), end + 100)
            context = chunk_text[context_start:context_end]
            contexts.append(context)
        
        return "\n---\n".join(contexts)
    
    def _merge_overlapping_spans(self, spans: List[Tuple[int, int]]) -> List[Tuple[int, int]]:
        """åˆå¹¶é‡å çš„ç‰‡æ®µ"""
        if not spans:
            return []
        
        # æŒ‰èµ·å§‹ä½ç½®æ’åº
        spans.sort(key=lambda x: x[0])
        
        merged = [spans[0]]
        for current in spans[1:]:
            last = merged[-1]
            
            # å¦‚æœå½“å‰ç‰‡æ®µä¸æœ€åä¸€ä¸ªç‰‡æ®µé‡å æˆ–ç›¸é‚»
            if current[0] <= last[1] + 50:  # å…è®¸50å­—ç¬¦çš„é—´éš”
                merged[-1] = (last[0], max(last[1], current[1]))
            else:
                merged.append(current)
        
        return merged
    
    def extract_with_llm(self, request: LLMRequest) -> Optional[LLMResponse]:
        """ä½¿ç”¨LLMè¿›è¡ŒæŠ½å–"""
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._generate_cache_key(request)
        if cache_key in self.cache:
            self.cache_hits += 1
            logger.info(f"LLMç¼“å­˜å‘½ä¸­: {request.field_name}")
            return self.cache[cache_key]
        
        self.total_calls += 1
        
        # æ˜¾ç¤ºLLMè°ƒç”¨è¿›åº¦
        logger.info(f"ğŸ”„ LLMè°ƒç”¨ #{self.total_calls}: å­—æ®µ={request.field_name}, ç±»å‹={request.field_type}")
        logger.info(f"ğŸ“¤ å‘é€å†…å®¹é•¿åº¦: {len(request.chunk_text)} å­—ç¬¦")
        
        try:
            if self.provider == "openai":
                response = self._call_openai(request)
            elif self.provider == "ollama":
                response = self._call_ollama(request)
            else:
                logger.error(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {self.provider}")
                return None
            
            if response:
                self.successful_calls += 1
                logger.info(f"âœ… LLMè°ƒç”¨æˆåŠŸ: {request.field_name} -> ç½®ä¿¡åº¦={response.confidence:.2f}")
                # ç¼“å­˜ç»“æœ
                self.cache[cache_key] = response
                return response
            else:
                self.failed_calls += 1
                logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {request.field_name}")
                return None
                
        except Exception as e:
            self.failed_calls += 1
            logger.error(f"âŒ LLMè°ƒç”¨å¼‚å¸¸: {request.field_name} - {e}")
            return None
    
    def _call_openai(self, request: LLMRequest) -> Optional[LLMResponse]:
        """è°ƒç”¨OpenAI API"""
        try:
            # æ„å»ºæç¤ºè¯
            prompt = self._build_openai_prompt(request)
            
            # æ˜¾ç¤ºå‘é€çš„æç¤ºè¯
            logger.info(f"ğŸ“¤ å‘é€ç»™OpenAIçš„æç¤ºè¯:")
            logger.info(f"å­—æ®µ: {request.field_name}")
            logger.info(f"å†…å®¹é¢„è§ˆ: {request.chunk_text[:200]}...")
            
            if self.debug_mode:
                logger.info("ğŸ” [è°ƒè¯•æ¨¡å¼] å®Œæ•´æç¤ºè¯:")
                logger.info("-" * 50)
                logger.info(prompt)
                logger.info("-" * 50)
            
            # è°ƒç”¨API
            response = self.client.ChatCompletion.create(
                model=self.model or "gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£ä¿¡æ¯æŠ½å–åŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=1000
            )
            
            # æ˜¾ç¤ºè¿”å›çš„å“åº”
            content = response.choices[0].message.content
            logger.info(f"ğŸ“¥ OpenAIè¿”å›å†…å®¹:")
            logger.info(f"å“åº”é•¿åº¦: {len(content)} å­—ç¬¦")
            logger.info(f"å“åº”é¢„è§ˆ: {content[:300]}...")
            
            if self.debug_mode:
                logger.info("ğŸ” [è°ƒè¯•æ¨¡å¼] å®Œæ•´å“åº”:")
                logger.info("-" * 50)
                logger.info(content)
                logger.info("-" * 50)
            
            # è§£æå“åº”
            return self._parse_openai_response(content, request.field_name)
            
        except Exception as e:
            logger.error(f"OpenAI APIè°ƒç”¨å¤±è´¥: {e}")
            return None
    
    def _call_ollama(self, request: LLMRequest) -> Optional[LLMResponse]:
        """è°ƒç”¨Ollama API"""
        try:
            # æ„å»ºæç¤ºè¯
            prompt = self._build_ollama_prompt(request)
            
            # æ˜¾ç¤ºå‘é€çš„æç¤ºè¯
            logger.info(f"ğŸ“¤ å‘é€ç»™Ollamaçš„æç¤ºè¯:")
            logger.info(f"å­—æ®µ: {request.field_name}")
            logger.info(f"å†…å®¹é¢„è§ˆ: {request.chunk_text[:200]}...")
            
            if self.debug_mode:
                logger.info("ğŸ” [è°ƒè¯•æ¨¡å¼] å®Œæ•´æç¤ºè¯:")
                logger.info("-" * 50)
                logger.info(prompt)
                logger.info("-" * 50)
            
            # è°ƒç”¨API
            response = self.client.chat(
                model=self.model or "deepseek-r1:32b",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£ä¿¡æ¯æŠ½å–åŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": prompt}
                ]
            )
            
            # æ˜¾ç¤ºè¿”å›çš„å“åº”
            content = response['message']['content']
            logger.info(f"ğŸ“¥ Ollamaè¿”å›å†…å®¹:")
            logger.info(f"å“åº”é•¿åº¦: {len(content)} å­—ç¬¦")
            logger.info(f"å“åº”é¢„è§ˆ: {content[:300]}...")
            
            if self.debug_mode:
                logger.info("ğŸ” [è°ƒè¯•æ¨¡å¼] å®Œæ•´å“åº”:")
                logger.info("-" * 50)
                logger.info(content)
                logger.info("-" * 50)
            
            # è§£æå“åº”
            return self._parse_ollama_response(content, request.field_name)
            
        except Exception as e:
            logger.error(f"Ollama APIè°ƒç”¨å¤±è´¥: {e}")
            return None
    
    def _build_openai_prompt(self, request: LLMRequest) -> str:
        """æ„å»ºOpenAIæç¤ºè¯"""
        prompt = f"""
è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–{request.field_name}å­—æ®µçš„ä¿¡æ¯ã€‚

æ–‡æœ¬å†…å®¹ï¼š
{request.chunk_text}

å­—æ®µç±»å‹ï¼š{request.field_type}
ä¸Šä¸‹æ–‡ï¼š{request.context or 'æ— '}

å¦‚æœå·²å­˜åœ¨å€¼ï¼š{', '.join(request.existing_values) if request.existing_values else 'æ— '}

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- extracted_values: æå–çš„å€¼åˆ—è¡¨
- confidence: ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰
- reasoning: æ¨ç†è¿‡ç¨‹
- evidence_spans: è¯æ®ç‰‡æ®µåˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«value, start, end, confidence

è¯·ç¡®ä¿è¿”å›çš„æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚
"""
        return prompt
    
    def _build_ollama_prompt(self, request: LLMRequest) -> str:
        """æ„å»ºOllamaæç¤ºè¯"""
        prompt = f"""
è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–{request.field_name}å­—æ®µçš„ä¿¡æ¯ã€‚

æ–‡æœ¬å†…å®¹ï¼š
{request.chunk_text}

å­—æ®µç±»å‹ï¼š{request.field_type}
ä¸Šä¸‹æ–‡ï¼š{request.context or 'æ— '}

å¦‚æœå·²å­˜åœ¨å€¼ï¼š{', '.join(request.existing_values) if request.existing_values else 'æ— '}

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- extracted_values: æå–çš„å€¼åˆ—è¡¨
- confidence: ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰
- reasoning: æ¨ç†è¿‡ç¨‹
- evidence_spans: è¯æ®ç‰‡æ®µåˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«value, start, end, confidence

è¯·ç¡®ä¿è¿”å›çš„æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚
"""
        return prompt
    
    def _parse_openai_response(self, content: str, field_name: str) -> Optional[LLMResponse]:
        """è§£æOpenAIå“åº”"""
        try:
            # å°è¯•æå–JSON
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            
            if json_start == -1 or json_end == 0:
                logger.warning("æœªæ‰¾åˆ°JSONå“åº”")
                return None
            
            json_str = content[json_start:json_end]
            data = json.loads(json_str)
            
            # æ„å»ºå“åº”
            evidence_spans = []
            for span_data in data.get('evidence_spans', []):
                evidence_span = EvidenceSpan(
                    value=span_data.get('value', ''),
                    start=span_data.get('start', 0),
                    end=span_data.get('end', 0),
                    confidence=span_data.get('confidence', 0.5),
                    source='llm',
                    pattern='openai'
                )
                evidence_spans.append(evidence_span)
            
            return LLMResponse(
                field_name=field_name,
                extracted_values=data.get('extracted_values', []),
                confidence=data.get('confidence', 0.5),
                reasoning=data.get('reasoning', ''),
                evidence_spans=evidence_spans
            )
            
        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æå¤±è´¥: {e}")
            return None
        except Exception as e:
            logger.error(f"å“åº”è§£æå¤±è´¥: {e}")
            return None
    
    def _parse_ollama_response(self, content: str, field_name: str) -> Optional[LLMResponse]:
        """è§£æOllamaå“åº”"""
        try:
            # å°è¯•æå–JSON
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            
            if json_start == -1 or json_end == 0:
                logger.warning("æœªæ‰¾åˆ°JSONå“åº”")
                return None
            
            json_str = content[json_start:json_end]
            data = json.loads(json_str)
            
            # æ„å»ºå“åº”
            evidence_spans = []
            for span_data in data.get('evidence_spans', []):
                evidence_span = EvidenceSpan(
                    value=span_data.get('value', ''),
                    start=span_data.get('start', 0),
                    end=span_data.get('end', 0),
                    confidence=span_data.get('confidence', 0.5),
                    source='llm',
                    pattern='ollama'
                )
                evidence_spans.append(evidence_span)
            
            return LLMResponse(
                field_name=field_name,
                extracted_values=data.get('extracted_values', []),
                confidence=data.get('confidence', 0.5),
                reasoning=data.get('reasoning', ''),
                evidence_spans=evidence_spans
            )
            
        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æå¤±è´¥: {e}")
            return None
        except Exception as e:
            logger.error(f"å“åº”è§£æå¤±è´¥: {e}")
            return None
    
    def _generate_cache_key(self, request: LLMRequest) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        content = f"{request.chunk_text}_{request.field_name}_{request.field_type}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def merge_llm_results(self, field: ExtractedField, llm_response: LLMResponse) -> ExtractedField:
        """åˆå¹¶LLMç»“æœåˆ°å­—æ®µ"""
        # æ·»åŠ LLMæå–çš„å€¼
        for value in llm_response.extracted_values:
            evidence_span = EvidenceSpan(
                value=value,
                start=0,  # LLMæ— æ³•æä¾›ç²¾ç¡®ä½ç½®
                end=0,
                confidence=llm_response.confidence,
                source='llm',
                pattern=f"{self.provider}:{self.model}"
            )
            field.values.append(evidence_span)
        
        # é‡æ–°è®¡ç®—ç½®ä¿¡åº¦
        if field.values:
            field.values.sort(key=lambda x: x.confidence, reverse=True)
            field.primary_value = field.values[0].value
            field.confidence = max(v.confidence for v in field.values)
        
        return field
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'provider': self.provider,
            'model': self.model,
            'total_calls': self.total_calls,
            'successful_calls': self.successful_calls,
            'failed_calls': self.failed_calls,
            'success_rate': self.successful_calls / self.total_calls if self.total_calls > 0 else 0,
            'cache_hits': self.cache_hits,
            'cache_size': len(self.cache)
        }
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        self.cache_hits = 0
        logger.info("LLMç¼“å­˜å·²æ¸…ç©º")
    
    def save_cache(self, filepath: str):
        """ä¿å­˜ç¼“å­˜åˆ°æ–‡ä»¶"""
        try:
            cache_data = {
                'cache': self.cache,
                'cache_hits': self.cache_hits,
                'statistics': self.get_statistics()
            }
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            logger.info(f"ç¼“å­˜å·²ä¿å­˜åˆ°: {filepath}")
        except Exception as e:
            logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def load_cache(self, filepath: str):
        """ä»æ–‡ä»¶åŠ è½½ç¼“å­˜"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            self.cache = cache_data.get('cache', {})
            self.cache_hits = cache_data.get('cache_hits', 0)
            logger.info(f"ç¼“å­˜å·²ä» {filepath} åŠ è½½")
        except Exception as e:
            logger.error(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}") 