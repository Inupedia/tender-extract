"""
CLIå‘½ä»¤è¡Œæ¥å£æ¨¡å—
"""
import os
import sys
import json
import time
import logging
from pathlib import Path
from typing import List, Optional
import typer
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.panel import Panel
from rich.text import Text

from .schema import ProcessingConfig, ExtractionResult, DocumentMetadata
from .preprocess import MarkdownPreprocessor
from .chunker import DocumentChunker, ChunkingConfig
from .rules import RuleExtractor
from .ner import NERExtractor
from .dedupe import DeduplicationEngine
from .llm_router import LLMRouter
from .merge import FieldMerger

app = typer.Typer(help="é¢å‘ä¸­æ–‡æ ‡ä¹¦çš„æ··åˆæŠ½å–æµæ°´çº¿")
console = Console()
logger = logging.getLogger(__name__)


class TenderExtractor:
    """æ ‡ä¹¦æŠ½å–å™¨ä¸»ç±»"""
    
    def __init__(self, config: ProcessingConfig, debug_mode: bool = False):
        self.config = config
        self.debug_mode = debug_mode
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.preprocessor = MarkdownPreprocessor()
        self.chunker = DocumentChunker(ChunkingConfig(
            max_tokens=config.max_chunk_tokens,
            overlap_tokens=config.overlap_tokens,
            min_chunk_tokens=200
        ))
        self.rule_extractor = RuleExtractor("config/example.yaml")
        self.ner_extractor = NERExtractor(use_foolnltk=config.use_ner)
        self.dedupe_engine = DeduplicationEngine(
            similarity_threshold=0.8,
            enable_lsh=config.enable_similarity_check
        )
        # è¯»å–é…ç½®æ–‡ä»¶è·å–Ollamaåœ°å€
        import yaml
        try:
            with open("config/example.yaml", 'r', encoding='utf-8') as f:
                yaml_config = yaml.safe_load(f)
                ollama_base_url = yaml_config.get('llm', {}).get('ollama_base_url', None)
        except Exception as e:
            console.print(f"[yellow]è­¦å‘Šï¼šæ— æ³•è¯»å–é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤Ollamaåœ°å€: {e}[/yellow]")
            ollama_base_url = None
        
        self.llm_router = LLMRouter(
            provider=config.llm_provider,
            model=config.llm_model,
            base_url=ollama_base_url,
            debug_mode=debug_mode
        )
        self.field_merger = FieldMerger()
    
    def extract_document(self, file_path: str) -> ExtractionResult:
        """æŠ½å–å•ä¸ªæ–‡æ¡£"""
        start_time = time.time()
        
        # è¯»å–æ–‡ä»¶
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # è·å–æ–‡ä»¶ä¿¡æ¯
        file_info = Path(file_path)
        file_size = file_info.stat().st_size
        
        # é¢„å¤„ç†
        structure_info = self.preprocessor.extract_structured_content(content)
        
        # åˆ‡ç‰‡
        chunks = self.chunker.chunk_document(content, file_info.name)
        
        # å»é‡
        if self.config.enable_dedupe:
            similarity_results = self.dedupe_engine.process_chunks(chunks)
            chunks = [chunk for i, chunk in enumerate(chunks) 
                     if not similarity_results[i].is_duplicate]
        
        # æ··åˆæŠ½å–æµæ°´çº¿ï¼šå…ˆç”¨è§„åˆ™å±‚åƒæ‰ç¡®å®šæ€§å­—æ®µï¼ŒåªæŠŠä½ç½®ä¿¡/å†²çªçš„å°ç‰‡æ®µè·¯ç”±ç»™LLM
        all_extractions = []
        llm_calls = 0
        
        for i, chunk in enumerate(chunks):
            # æ˜¾ç¤ºchunkå¤„ç†è¿›åº¦
            logger.info(f"ğŸ“„ å¤„ç†åˆ‡ç‰‡ {i+1}/{len(chunks)}: é•¿åº¦={len(chunk.content)}å­—ç¬¦")
            
            # è®¡ç®—chunkåœ¨å®Œæ•´æ–‡æ¡£ä¸­çš„åç§»é‡
            chunk_offset = self._find_chunk_offset(content, chunk.content, chunk.start_line)
            
            # 1. è§„åˆ™æŠ½å–ï¼ˆé«˜ååï¼‰
            rule_fields = self.rule_extractor.extract_fields(chunk.content)
            logger.info(f"   ğŸ“‹ è§„åˆ™æŠ½å–å®Œæˆ: æ‰¾åˆ° {len(rule_fields)} ä¸ªå­—æ®µ")
            
            # 2. NERæŠ½å–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.config.use_ner:
                ner_fields = self.ner_extractor.extract_entities(chunk.content, content)
                logger.info(f"   ğŸ·ï¸  NERæŠ½å–å®Œæˆ: æ‰¾åˆ° {len(ner_fields)} ä¸ªå®ä½“")
                # è°ƒæ•´NERç»“æœçš„ä½ç½®ä¿¡æ¯å¹¶æ·»åŠ å¼•ç”¨æ®µè½
                for field_name, field in ner_fields.items():
                    for evidence in field.values:
                        # è°ƒæ•´ä½ç½®ä¿¡æ¯åˆ°å®Œæ•´æ–‡æ¡£ä¸­çš„ç»å¯¹ä½ç½®
                        evidence.start += chunk_offset
                        evidence.end += chunk_offset
                        
                        # æ·»åŠ å¼•ç”¨æ®µè½ - åŸºäºå†…å®¹åŒ¹é…è€Œä¸æ˜¯ä½ç½®
                        evidence.ref = self._find_ref_by_content(content, evidence.value)
                rule_fields = self.ner_extractor.merge_with_rules(ner_fields, rule_fields)
            
            # 3. è°ƒæ•´ä½ç½®ä¿¡æ¯å¹¶æ·»åŠ å¼•ç”¨æ®µè½
            for field_name, field in rule_fields.items():
                for evidence in field.values:
                    # è°ƒæ•´ä½ç½®ä¿¡æ¯åˆ°å®Œæ•´æ–‡æ¡£ä¸­çš„ç»å¯¹ä½ç½®
                    evidence.start += chunk_offset
                    evidence.end += chunk_offset
                    
                    # æ·»åŠ å¼•ç”¨æ®µè½ - åŸºäºå†…å®¹åŒ¹é…è€Œä¸æ˜¯ä½ç½®
                    evidence.ref = self._find_ref_by_content(content, evidence.value)
            
            # 4. æŒ‰éœ€LLMè·¯ç”±ï¼ˆä»…å¤„ç†ä½ç½®ä¿¡/å†²çªå­—æ®µï¼‰
            llm_fields_count = 0
            for field_name, field in rule_fields.items():
                if self.llm_router.should_use_llm(field, self.config.confidence_threshold):
                    llm_fields_count += 1
                    logger.info(f"   ğŸ¤– ä½¿ç”¨LLMå¤„ç†å­—æ®µ: {field_name} (ç½®ä¿¡åº¦={field.confidence:.2f})")
                    
                    # è·å–æœ€å°è¯æ®ç‰‡æ®µ
                    minimal_context = self.llm_router.get_minimal_evidence_context(field, chunk.content)
                    
                    from .schema import LLMRequest
                    llm_request = LLMRequest(
                        chunk_text=minimal_context,  # åªå‘é€æœ€å°è¯æ®ç‰‡æ®µ
                        field_name=field_name,
                        field_type=field.field_type
                    )
                    
                    llm_response = self.llm_router.extract_with_llm(llm_request)
                    if llm_response:
                        field = self.llm_router.merge_llm_results(field, llm_response)
                        llm_calls += 1
                        logger.info(f"   âœ… LLMå¤„ç†å®Œæˆ: {field_name} -> æ–°ç½®ä¿¡åº¦={field.confidence:.2f}")
                    else:
                        logger.warning(f"   âš ï¸  LLMå¤„ç†å¤±è´¥: {field_name}")
            
            if llm_fields_count > 0:
                logger.info(f"   ğŸ¤– æœ¬åˆ‡ç‰‡LLMå¤„ç†: {llm_fields_count} ä¸ªå­—æ®µ")
            
            all_extractions.append(rule_fields)
        
        # è®°å½•LLMè°ƒç”¨æ¬¡æ•°
        self.llm_calls = llm_calls
        logger.info(f"ğŸ“Š æ€»LLMè°ƒç”¨æ¬¡æ•°: {llm_calls}")
        
        # åˆå¹¶ç»“æœ
        merged_fields = self._merge_all_extractions(all_extractions)
        
        # è§£å†³å†²çª
        resolved_fields = self.field_merger.resolve_conflicts(merged_fields)
        
        # æ„å»ºç»“æœ
        processing_time = time.time() - start_time
        
        metadata = DocumentMetadata(
            filename=file_info.name,
            file_size=file_size,
            total_lines=structure_info['total_lines'],
            total_chunks=len(chunks),
            processing_time=processing_time,
            extraction_stats=self._get_extraction_stats(resolved_fields)
        )
        
        return ExtractionResult(
            metadata=metadata,
            fields=resolved_fields,
            chunks_processed=len(chunks),
            llm_calls=getattr(self, 'llm_calls', 0),
            cache_hits=self.llm_router.cache_hits,
            errors=[],
            warnings=[]
        )
    
    def _merge_all_extractions(self, extractions: List[dict]) -> dict:
        """åˆå¹¶æ‰€æœ‰æŠ½å–ç»“æœ"""
        merged_fields = {}
        
        for extraction in extractions:
            for field_name, field in extraction.items():
                if field_name not in merged_fields:
                    merged_fields[field_name] = field
                else:
                    # åˆå¹¶å­—æ®µ
                    existing_field = merged_fields[field_name]
                    existing_field.values.extend(field.values)
                    
                    # é‡æ–°è®¡ç®—ç½®ä¿¡åº¦
                    if existing_field.values:
                        existing_field.values.sort(key=lambda x: x.confidence, reverse=True)
                        existing_field.primary_value = existing_field.values[0].value
                        existing_field.confidence = max(v.confidence for v in existing_field.values)
        
        return merged_fields
    
    def _get_extraction_stats(self, fields: dict) -> dict:
        """è·å–æŠ½å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_fields': len(fields),
            'fields_by_type': {},
            'avg_confidence': 0.0
        }
        
        total_confidence = 0.0
        
        for field in fields.values():
            field_type = field.field_type
            if field_type not in stats['fields_by_type']:
                stats['fields_by_type'][field_type] = 0
            stats['fields_by_type'][field_type] += 1
            
            total_confidence += field.confidence
        
        if stats['total_fields'] > 0:
            stats['avg_confidence'] = total_confidence / stats['total_fields']
        
        # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯åŸºæœ¬ç±»å‹
        for key, value in stats['fields_by_type'].items():
            stats['fields_by_type'][key] = int(value)
        
        return stats
    
    def _find_chunk_offset(self, full_content: str, chunk_content: str, start_line: int) -> int:
        """
        è®¡ç®—chunkåœ¨å®Œæ•´æ–‡æ¡£ä¸­çš„å­—ç¬¦åç§»é‡
        
        Args:
            full_content: å®Œæ•´æ–‡æ¡£å†…å®¹
            chunk_content: chunkå†…å®¹
            start_line: chunkçš„èµ·å§‹è¡Œå·
            
        Returns:
            å­—ç¬¦åç§»é‡
        """
        # æ–¹æ³•1ï¼šåŸºäºå†…å®¹åŒ¹é…ï¼ˆæœ€å‡†ç¡®ï¼‰
        try:
            pos = full_content.find(chunk_content)
            if pos != -1:
                return pos
        except:
            pass
        
        # æ–¹æ³•2ï¼šåŸºäºè¡Œå·è®¡ç®—ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        if start_line > 1:
            lines = full_content.split('\n')
            if start_line <= len(lines):
                # è®¡ç®—å‰é¢æ‰€æœ‰è¡Œçš„å­—ç¬¦æ•°
                offset = sum(len(line) + 1 for line in lines[:start_line - 1])  # +1 for newline
                return offset
        
        # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œè¿”å›0
        return 0
    
    def _find_ref_by_content(self, full_content: str, entity_value: str) -> str:
        """
        åŸºäºå†…å®¹åŒ¹é…æŸ¥æ‰¾å¼•ç”¨æ®µè½ï¼Œå¢å¼ºç« èŠ‚ä¿¡æ¯
        
        Args:
            full_content: å®Œæ•´æ–‡æ¡£å†…å®¹
            entity_value: å®ä½“å€¼
            
        Returns:
            åŒ…å«å®ä½“çš„å¼•ç”¨æ®µè½ï¼Œå¢å¼ºç« èŠ‚ä¿¡æ¯
        """
        if not entity_value or not entity_value.strip():
            return ""
        
        # åœ¨å®Œæ•´æ–‡æ¡£ä¸­æŸ¥æ‰¾å®ä½“
        pos = full_content.find(entity_value)
        if pos == -1:
            return entity_value  # å¦‚æœæ‰¾ä¸åˆ°ï¼Œè¿”å›å®ä½“æœ¬èº«
        
        # æ„å»ºç« èŠ‚æ ‘ä»¥è·å–ç« èŠ‚ä¿¡æ¯
        from .preprocess import MarkdownPreprocessor
        preprocessor = MarkdownPreprocessor()
        chapter_tree = preprocessor.build_chapter_tree(full_content)
        
        # è·å–å®ä½“æ‰€åœ¨ä½ç½®çš„ç« èŠ‚è·¯å¾„
        entity_line = full_content[:pos].count('\n') + 1
        chapter_path = preprocessor.get_chapter_path(entity_line)
        
        # ä»å®ä½“ä½ç½®å¼€å§‹æŸ¥æ‰¾æ®µè½
        start_pos = pos
        end_pos = pos + len(entity_value)
        
        # å‘å‰æŸ¥æ‰¾æ®µè½å¼€å§‹
        paragraph_start = start_pos
        while paragraph_start > 0:
            char = full_content[paragraph_start - 1]
            if char in ['\n', '\r']:
                # æ£€æŸ¥æ˜¯å¦æ˜¯è¿ç»­çš„ç©ºè¡Œï¼ˆæ®µè½åˆ†éš”ï¼‰
                if paragraph_start > 1:
                    prev_char = full_content[paragraph_start - 2]
                    if prev_char in ['\n', '\r']:
                        break
                # å•ä¸ªæ¢è¡Œç¬¦ï¼Œç»§ç»­å‘å‰æœç´¢
                paragraph_start -= 1
            else:
                paragraph_start -= 1
            
            # é™åˆ¶å‘å‰æœç´¢çš„èŒƒå›´
            if start_pos - paragraph_start > 500:
                break
        
        # å‘åæŸ¥æ‰¾æ®µè½ç»“æŸ
        paragraph_end = end_pos
        while paragraph_end < len(full_content):
            char = full_content[paragraph_end]
            if char in ['\n', '\r']:
                # æ£€æŸ¥æ˜¯å¦æ˜¯è¿ç»­çš„ç©ºè¡Œï¼ˆæ®µè½åˆ†éš”ï¼‰
                if paragraph_end + 1 < len(full_content):
                    next_char = full_content[paragraph_end + 1]
                    if next_char in ['\n', '\r']:
                        break
                # å•ä¸ªæ¢è¡Œç¬¦ï¼Œç»§ç»­å‘åæœç´¢
                paragraph_end += 1
            else:
                paragraph_end += 1
            
            # é™åˆ¶å‘åæœç´¢çš„èŒƒå›´
            if paragraph_end - end_pos > 500:
                break
        
        # æå–æ®µè½å†…å®¹
        paragraph = full_content[paragraph_start:paragraph_end].strip()
        
        # æ¸…ç†æ®µè½å†…å®¹
        paragraph = self._clean_paragraph_content(paragraph)
        
        # ç¡®ä¿æ®µè½åŒ…å«å®ä½“
        if entity_value not in paragraph:
            # å¦‚æœæ¸…ç†åä¸åŒ…å«å®ä½“ï¼Œè¿”å›åŸå§‹æ®µè½
            paragraph = full_content[paragraph_start:paragraph_end].strip()
        
        # å¢å¼ºrefå­—æ®µï¼šæ·»åŠ ç« èŠ‚ä¿¡æ¯
        enhanced_ref = self._enhance_ref_with_chapter_info(paragraph, chapter_path, entity_value)
        
        return enhanced_ref
    
    def _enhance_ref_with_chapter_info(self, paragraph: str, chapter_path: List[str], entity_value: str) -> str:
        """
        å¢å¼ºå¼•ç”¨æ®µè½ï¼Œæ·»åŠ ç« èŠ‚ä¿¡æ¯
        
        Args:
            paragraph: åŸå§‹æ®µè½
            chapter_path: ç« èŠ‚è·¯å¾„
            entity_value: å®ä½“å€¼
            
        Returns:
            å¢å¼ºåçš„å¼•ç”¨æ®µè½
        """
        if not chapter_path:
            return paragraph
        
        # æ„å»ºç« èŠ‚ä¿¡æ¯
        chapter_info = " â†’ ".join(chapter_path)
        
        # å¦‚æœæ®µè½å¤ªçŸ­ï¼Œç›´æ¥è¿”å›å¸¦ç« èŠ‚ä¿¡æ¯çš„æ®µè½
        if len(paragraph.strip()) < 50:
            return f"[ç« èŠ‚: {chapter_info}] {paragraph}"
        
        # å°è¯•æ‰¾åˆ°åŒ…å«å®ä½“çš„å®Œæ•´å¥å­
        sentences = self._split_into_sentences(paragraph)
        relevant_sentences = []
        
        for sentence in sentences:
            if entity_value in sentence:
                relevant_sentences.append(sentence)
        
        # å¦‚æœæ‰¾åˆ°äº†ç›¸å…³å¥å­ï¼Œä½¿ç”¨å®ƒä»¬ï¼›å¦åˆ™ä½¿ç”¨æ•´ä¸ªæ®µè½
        if relevant_sentences:
            content = " ".join(relevant_sentences)
        else:
            content = paragraph
        
        # æ„å»ºå¢å¼ºçš„å¼•ç”¨ï¼ŒåŒ…å«æ›´ä¸°å¯Œçš„ç« èŠ‚ä¿¡æ¯
        enhanced_ref = f"[ç« èŠ‚è·¯å¾„: {chapter_info}] {content}"
        
        # å¦‚æœç« èŠ‚è·¯å¾„åŒ…å«å…³é”®ä¿¡æ¯ï¼Œè¿›ä¸€æ­¥æ ‡æ³¨
        key_sections = ['æŠ•æ ‡å‡½', 'æŠ•æ ‡æ–‡ä»¶', 'æ‹›æ ‡', 'è¯„æ ‡', 'åˆåŒ', 'ä¿è¯é‡‘', 'æŠ¥ä»·']
        for key_section in key_sections:
            if any(key_section in chapter for chapter in chapter_path):
                enhanced_ref = f"[å…³é”®ç« èŠ‚: {key_section} | ç« èŠ‚è·¯å¾„: {chapter_info}] {content}"
                break
        
        return enhanced_ref
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """
        å°†æ–‡æœ¬åˆ†å‰²æˆå¥å­
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            å¥å­åˆ—è¡¨
        """
        import re
        
        # ä¸­æ–‡å¥å­åˆ†å‰²æ¨¡å¼
        sentence_pattern = r'[ã€‚ï¼ï¼Ÿï¼›\n]+'
        sentences = re.split(sentence_pattern, text)
        
        # è¿‡æ»¤ç©ºå¥å­å¹¶æ¸…ç†
        cleaned_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if sentence and len(sentence) > 5:  # è¿‡æ»¤å¤ªçŸ­çš„å¥å­
                cleaned_sentences.append(sentence)
        
        return cleaned_sentences
    
    def _is_numbered_list_item(self, entity_value: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯åºå·åˆ—è¡¨é¡¹"""
        # æ£€æŸ¥æ˜¯å¦ä»¥æ•°å­—å¼€å¤´ï¼Œåé¢è·Ÿç€ç©ºæ ¼æˆ–ç‚¹
        import re
        return bool(re.match(r'^\d+[.\s]', entity_value.strip()))
    
    def _clean_paragraph_content(self, paragraph: str) -> str:
        """
        æ¸…ç†æ®µè½å†…å®¹ï¼Œç§»é™¤è¡¨æ ¼ç­‰æ— å…³å†…å®¹
        
        Args:
            paragraph: åŸå§‹æ®µè½å†…å®¹
            
        Returns:
            æ¸…ç†åçš„æ®µè½å†…å®¹
        """
        if not paragraph:
            return paragraph
        
        # å¦‚æœæ®µè½å¤ªçŸ­ï¼Œç›´æ¥è¿”å›
        if len(paragraph.strip()) < 10:
            return paragraph.strip()
        
        # ç§»é™¤è¡¨æ ¼å†…å®¹ï¼ˆä»¥|å¼€å¤´çš„è¡Œï¼‰
        lines = paragraph.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            # è·³è¿‡è¡¨æ ¼è¡Œ
            if line.startswith('|') or line.startswith('---') or line.startswith('+'):
                continue
            # è·³è¿‡ç©ºè¡Œ
            if not line:
                continue
            # è·³è¿‡åªåŒ…å«ç‰¹æ®Šå­—ç¬¦çš„è¡Œ
            if line.replace('-', '').replace('_', '').replace('=', '').replace('*', '').strip() == '':
                continue
            cleaned_lines.append(line)
        
        # é‡æ–°ç»„åˆæ®µè½
        cleaned_paragraph = '\n'.join(cleaned_lines)
        
        # ç§»é™¤è¿‡å¤šçš„ç©ºç™½å­—ç¬¦
        import re
        cleaned_paragraph = re.sub(r'\s+', ' ', cleaned_paragraph)
        
        # å¦‚æœæ¸…ç†åå†…å®¹å¤ªçŸ­ï¼Œè¿”å›åŸå§‹å†…å®¹
        if len(cleaned_paragraph.strip()) < 10:
            return paragraph.strip()
        
        return cleaned_paragraph.strip()


@app.command()
def extract(
    input_path: str = typer.Argument(..., help="è¾“å…¥æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„"),
    out: str = typer.Option("./out", "--out", "-o", help="è¾“å‡ºç›®å½•"),
    config: str = typer.Option("./config/example.yaml", "--config", "-c", help="é…ç½®æ–‡ä»¶è·¯å¾„"),
    use_ner: bool = typer.Option(False, "--use-ner", help="å¯ç”¨NER"),
    llm: str = typer.Option("none", "--llm", help="LLMæä¾›å•†ï¼šnone/ollama/openai"),
    model: Optional[str] = typer.Option(None, "--model", "-m", help="LLMæ¨¡å‹åç§°"),
    pattern: str = typer.Option("*.md", "--pattern", "-p", help="æ–‡ä»¶åŒ¹é…æ¨¡å¼"),
    cache_dir: str = typer.Option(".cache", "--cache-dir", help="ç¼“å­˜ç›®å½•"),
    verbose: bool = typer.Option(False, "--verbose", "-v", help="è¯¦ç»†è¾“å‡º"),
    debug: bool = typer.Option(False, "--debug", help="LLMè°ƒè¯•æ¨¡å¼ï¼Œæ˜¾ç¤ºå®Œæ•´æç¤ºè¯å’Œå“åº”")
):
    """æŠ½å–æ–‡æ¡£ä¿¡æ¯"""
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if verbose:
        import logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        console.print("[yellow]è¯¦ç»†æ¨¡å¼å·²å¯ç”¨ï¼Œå°†æ˜¾ç¤ºLLMå¤„ç†è¯¦æƒ…[/yellow]")
    
    if debug:
        import logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        console.print("[red]ğŸ” LLMè°ƒè¯•æ¨¡å¼å·²å¯ç”¨ï¼Œå°†æ˜¾ç¤ºå®Œæ•´æç¤ºè¯å’Œå“åº”[/red]")
    
    # æ£€æŸ¥è¾“å…¥è·¯å¾„
    input_path = Path(input_path)
    if not input_path.exists():
        console.print(f"[red]é”™è¯¯ï¼šè¾“å…¥è·¯å¾„ä¸å­˜åœ¨ {input_path}[/red]")
        sys.exit(1)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    out_path = Path(out)
    out_path.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºç¼“å­˜ç›®å½•
    cache_path = Path(cache_dir)
    cache_path.mkdir(parents=True, exist_ok=True)
    
    # æ„å»ºé…ç½®
    processing_config = ProcessingConfig(
        use_ner=use_ner,
        llm_provider=llm,
        llm_model=model,
        confidence_threshold=0.7,
        max_chunk_tokens=800,
        overlap_tokens=100,
        cache_dir=str(cache_path),
        enable_dedupe=True,
        enable_similarity_check=True
    )
    
    # åˆå§‹åŒ–æŠ½å–å™¨
    extractor = TenderExtractor(processing_config, debug_mode=debug)
    
    # è·å–æ–‡ä»¶åˆ—è¡¨
    if input_path.is_file():
        files = [input_path]
    else:
        files = list(input_path.glob(pattern))
    
    if not files:
        console.print(f"[yellow]è­¦å‘Šï¼šæœªæ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶ {pattern}[/yellow]")
        return
    
    console.print(f"[green]æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶[/green]")
    if llm != "none":
        console.print(f"[blue]LLMé…ç½®: {llm} - {model or 'é»˜è®¤æ¨¡å‹'}[/blue]")
    
    # å¤„ç†æ–‡ä»¶
    results = []
    total_llm_calls = 0
    total_cache_hits = 0
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console
    ) as progress:
        
        for i, file_path in enumerate(files):
            task = progress.add_task(f"å¤„ç† {file_path.name}...", total=None)
            
            try:
                console.print(f"\n[bold cyan]å¤„ç†æ–‡ä»¶ {i+1}/{len(files)}: {file_path.name}[/bold cyan]")
                result = extractor.extract_document(str(file_path))
                results.append(result)
                
                # ç´¯è®¡LLMç»Ÿè®¡
                total_llm_calls += result.llm_calls
                total_cache_hits += result.cache_hits
                
                progress.update(task, description=f"å®Œæˆ {file_path.name}")
                
                # æ˜¾ç¤ºæ–‡ä»¶å¤„ç†ç»“æœ
                console.print(f"[green]âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {file_path.name}[/green]")
                console.print(f"   ğŸ“Š æŠ½å–å­—æ®µ: {result.metadata.extraction_stats.get('total_fields', 0)} ä¸ª")
                console.print(f"   ğŸ¤– LLMè°ƒç”¨: {result.llm_calls} æ¬¡")
                console.print(f"   ğŸ’¾ ç¼“å­˜å‘½ä¸­: {result.cache_hits} æ¬¡")
                console.print(f"   â±ï¸  å¤„ç†æ—¶é—´: {result.metadata.processing_time:.2f} ç§’")
                
            except Exception as e:
                progress.update(task, description=f"å¤±è´¥ {file_path.name}")
                console.print(f"[red]âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}[/red]")
    
    # æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡
    if llm != "none":
        console.print(f"\n[bold]ğŸ“ˆ æ€»ä½“LLMç»Ÿè®¡[/bold]")
        console.print(f"   ğŸ¤– æ€»LLMè°ƒç”¨: {total_llm_calls} æ¬¡")
        console.print(f"   ğŸ’¾ æ€»ç¼“å­˜å‘½ä¸­: {total_cache_hits} æ¬¡")
        if total_llm_calls > 0:
            cache_rate = total_cache_hits / (total_llm_calls + total_cache_hits) * 100
            console.print(f"   ğŸ“Š ç¼“å­˜å‘½ä¸­ç‡: {cache_rate:.1f}%")
    
    # ä¿å­˜ç»“æœ - åªç”Ÿæˆä¸€ä¸ªJSONæ–‡ä»¶
    for result in results:
        output_file = out_path / f"{result.metadata.filename}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result.dict(), f, ensure_ascii=False, indent=2)
        
        console.print(f"[green]ğŸ’¾ ç»“æœå·²ä¿å­˜: {output_file}[/green]")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    _display_results_summary(results)


@app.command()
def info(
    file_path: str = typer.Argument(..., help="æ–‡ä»¶è·¯å¾„")
):
    """æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯"""
    
    file_path = Path(file_path)
    if not file_path.exists():
        console.print(f"[red]é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨ {file_path}[/red]")
        sys.exit(1)
    
    # è¯»å–æ–‡ä»¶
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # é¢„å¤„ç†
    preprocessor = MarkdownPreprocessor()
    structure_info = preprocessor.extract_structured_content(content)
    
    # æ˜¾ç¤ºä¿¡æ¯
    console.print(Panel.fit(
        f"[bold]æ–‡ä»¶ä¿¡æ¯[/bold]\n"
        f"æ–‡ä»¶åï¼š{file_path.name}\n"
        f"å¤§å°ï¼š{file_path.stat().st_size:,} å­—èŠ‚\n"
        f"æ€»è¡Œæ•°ï¼š{structure_info['total_lines']:,}\n"
        f"æ€»ç« èŠ‚æ•°ï¼š{structure_info['total_chapters']}\n"
        f"ä¼°ç®—tokenæ•°ï¼š{structure_info['content_stats']['estimated_tokens']:,}",
        title="æ–‡æ¡£åˆ†æ"
    ))
    
    # æ˜¾ç¤ºå…³é”®ç« èŠ‚
    if structure_info['key_sections']:
        table = Table(title="å…³é”®ç« èŠ‚")
        table.add_column("æ ‡é¢˜", style="cyan")
        table.add_column("çº§åˆ«", style="magenta")
        table.add_column("è¡Œå·èŒƒå›´", style="green")
        table.add_column("å†…å®¹é¢„è§ˆ", style="yellow")
        
        for section in structure_info['key_sections'][:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            table.add_row(
                section['title'],
                str(section['level']),
                f"{section['start_line']}-{section['end_line']}",
                section['content_preview'][:50] + "..."
            )
        
        console.print(table)


@app.command()
def test(
    file_path: str = typer.Argument(..., help="æµ‹è¯•æ–‡ä»¶è·¯å¾„"),
    config: str = typer.Option("./config/example.yaml", "--config", "-c", help="é…ç½®æ–‡ä»¶è·¯å¾„")
):
    """æµ‹è¯•æŠ½å–åŠŸèƒ½"""
    
    file_path = Path(file_path)
    if not file_path.exists():
        console.print(f"[red]é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨ {file_path}[/red]")
        sys.exit(1)
    
    # è¯»å–æ–‡ä»¶
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æµ‹è¯•å„ä¸ªç»„ä»¶
    console.print("[bold]æµ‹è¯•å„ä¸ªç»„ä»¶...[/bold]")
    
    # 1. æµ‹è¯•é¢„å¤„ç†
    console.print("\n[cyan]1. æµ‹è¯•é¢„å¤„ç†[/cyan]")
    preprocessor = MarkdownPreprocessor()
    structure_info = preprocessor.extract_structured_content(content)
    console.print(f"âœ“ é¢„å¤„ç†å®Œæˆï¼Œå…± {structure_info['total_chapters']} ä¸ªç« èŠ‚")
    
    # 2. æµ‹è¯•åˆ‡ç‰‡
    console.print("\n[cyan]2. æµ‹è¯•åˆ‡ç‰‡[/cyan]")
    chunker = DocumentChunker(ChunkingConfig())
    chunks = chunker.chunk_document(content, file_path.name)
    console.print(f"âœ“ åˆ‡ç‰‡å®Œæˆï¼Œå…± {len(chunks)} ä¸ªåˆ‡ç‰‡")
    
    # 3. æµ‹è¯•è§„åˆ™æŠ½å–
    console.print("\n[cyan]3. æµ‹è¯•è§„åˆ™æŠ½å–[/cyan]")
    rule_extractor = RuleExtractor(config)
    
    # æµ‹è¯•å‰å‡ ä¸ªåˆ‡ç‰‡
    total_fields = 0
    for i, chunk in enumerate(chunks[:3]):  # åªæµ‹è¯•å‰3ä¸ªåˆ‡ç‰‡
        fields = rule_extractor.extract_fields(chunk.content, content)
        total_fields += len(fields)
        console.print(f"  åˆ‡ç‰‡ {i+1}: æŠ½å–åˆ° {len(fields)} ä¸ªå­—æ®µ")
    
    console.print(f"âœ“ è§„åˆ™æŠ½å–å®Œæˆï¼Œå‰3ä¸ªåˆ‡ç‰‡å…±æŠ½å– {total_fields} ä¸ªå­—æ®µ")
    
    # 4. æµ‹è¯•NERï¼ˆå¦‚æœå¯ç”¨ï¼‰
    console.print("\n[cyan]4. æµ‹è¯•NER[/cyan]")
    ner_extractor = NERExtractor()
    if ner_extractor.use_foolnltk or ner_extractor.use_jieba:
        ner_fields = ner_extractor.extract_entities(content[:1000], content)  # åªæµ‹è¯•å‰1000å­—ç¬¦
        console.print(f"âœ“ NERå®Œæˆï¼ŒæŠ½å–åˆ° {len(ner_fields)} ä¸ªå®ä½“ç±»å‹")
    else:
        console.print("âš  NERç»„ä»¶ä¸å¯ç”¨")
    
    console.print("\n[green]âœ“ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼[/green]")


def _display_results_summary(results: List[ExtractionResult]):
    """æ˜¾ç¤ºç»“æœæ‘˜è¦"""
    
    if not results:
        return
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_files = len(results)
    total_fields = sum(len(r.fields) for r in results)
    total_llm_calls = sum(r.llm_calls for r in results)
    total_cache_hits = sum(r.cache_hits for r in results)
    avg_processing_time = sum(r.metadata.processing_time for r in results) / total_files
    
    # å­—æ®µç±»å‹ç»Ÿè®¡
    field_types = {}
    for result in results:
        for field in result.fields.values():
            field_type = field.field_type
            if field_type not in field_types:
                field_types[field_type] = 0
            field_types[field_type] += 1
    
    # æ˜¾ç¤ºæ‘˜è¦
    console.print(Panel.fit(
        f"[bold]å¤„ç†å®Œæˆï¼[/bold]\n"
        f"æ–‡ä»¶æ•°ï¼š{total_files}\n"
        f"æ€»å­—æ®µæ•°ï¼š{total_fields}\n"
        f"LLMè°ƒç”¨æ¬¡æ•°ï¼š{total_llm_calls}\n"
        f"ç¼“å­˜å‘½ä¸­æ¬¡æ•°ï¼š{total_cache_hits}\n"
        f"å¹³å‡å¤„ç†æ—¶é—´ï¼š{avg_processing_time:.2f}ç§’",
        title="å¤„ç†æ‘˜è¦"
    ))
    
    # æ˜¾ç¤ºå­—æ®µç±»å‹åˆ†å¸ƒ
    if field_types:
        table = Table(title="å­—æ®µç±»å‹åˆ†å¸ƒ")
        table.add_column("å­—æ®µç±»å‹", style="cyan")
        table.add_column("æ•°é‡", style="magenta")
        
        for field_type, count in sorted(field_types.items(), key=lambda x: x[1], reverse=True):
            table.add_row(field_type, str(count))
        
        console.print(table)


def _generate_summary(extraction_result: ExtractionResult, output_path: str):
    """ç”Ÿæˆæ ‡ä¹¦å…³é”®ä¿¡æ¯æ‘˜è¦"""
    # å…³é”®å­—æ®µä¼˜å…ˆçº§
    priority_fields = [
        'project_name', 'bidder', 'tenderer', 'bid_amount', 
        'deposit', 'project_manager', 'legal_representative',
        'bid_date', 'project_number', 'contact_info',
        # å›´ä¸²æ ‡æ£€æµ‹å…³é”®å­—æ®µ
        'shareholder_info', 'subsidiary_info', 'business_license',
        'registered_capital', 'establishment_date', 'registered_address',
        'business_scope', 'qualification_cert', 'performance_record',
        'financial_info', 'bank_account', 'joint_venture',
        'technical_staff', 'equipment_info', 'bidding_consortium'
    ]
    
    field_display_names = {
        'project_name': 'é¡¹ç›®åç§°',
        'bidder': 'æŠ•æ ‡äºº',
        'tenderer': 'æ‹›æ ‡äºº',
        'bid_amount': 'æŠ•æ ‡é‡‘é¢',
        'deposit': 'ä¿è¯é‡‘',
        'project_manager': 'é¡¹ç›®è´Ÿè´£äºº',
        'legal_representative': 'æ³•å®šä»£è¡¨äºº',
        'bid_date': 'æŠ•æ ‡æ—¥æœŸ',
        'project_number': 'é¡¹ç›®ç¼–å·',
        'contact_info': 'è”ç³»æ–¹å¼',
        # å›´ä¸²æ ‡æ£€æµ‹å…³é”®å­—æ®µ
        'shareholder_info': 'è‚¡ä¸œä¿¡æ¯',
        'subsidiary_info': 'å…³è”å…¬å¸',
        'business_license': 'è¥ä¸šæ‰§ç…§',
        'registered_capital': 'æ³¨å†Œèµ„æœ¬',
        'establishment_date': 'æˆç«‹æ—¥æœŸ',
        'registered_address': 'æ³¨å†Œåœ°å€',
        'business_scope': 'ç»è¥èŒƒå›´',
        'qualification_cert': 'èµ„è´¨è¯ä¹¦',
        'performance_record': 'ä¸šç»©è®°å½•',
        'financial_info': 'è´¢åŠ¡ä¿¡æ¯',
        'bank_account': 'é“¶è¡Œè´¦æˆ·',
        'joint_venture': 'è”åˆä½“',
        'technical_staff': 'æŠ€æœ¯äººå‘˜',
        'equipment_info': 'è®¾å¤‡ä¿¡æ¯',
        'bidding_consortium': 'æŠ•æ ‡è”åˆä½“',
        'person': 'ç›¸å…³äººå‘˜',
        'company': 'å…¬å¸ä¿¡æ¯',
        'organization': 'ç»„ç»‡æœºæ„',
        'location': 'åœ°ç‚¹ä¿¡æ¯',
    }
    
    summary = {
        "æ–‡æ¡£ä¿¡æ¯": {
            "æ–‡ä»¶å": extraction_result.metadata.filename,
            "æ–‡ä»¶å¤§å°": f"{extraction_result.metadata.file_size / 1024:.1f}KB",
            "å¤„ç†æ—¶é—´": f"{extraction_result.metadata.processing_time:.2f}ç§’",
            "æŠ½å–å­—æ®µæ•°": extraction_result.metadata.extraction_stats.get('total_fields', 0),
            "å¹³å‡ç½®ä¿¡åº¦": f"{extraction_result.metadata.extraction_stats.get('avg_confidence', 0):.2f}"
        },
        "å…³é”®ä¿¡æ¯": {}
    }
    
    # æå–å…³é”®å­—æ®µ
    for field_name in priority_fields:
        if field_name in extraction_result.fields:
            field = extraction_result.fields[field_name]
            if field.values:
                best_value = field.values[0]
                summary["å…³é”®ä¿¡æ¯"][field_display_names.get(field_name, field_name)] = {
                    "å€¼": best_value.value,
                    "ç½®ä¿¡åº¦": f"{best_value.confidence:.2f}",
                    "æ¥æº": best_value.source
                }
    
    # æ·»åŠ å…¶ä»–é‡è¦å­—æ®µ
    other_important_fields = ['person', 'company', 'organization', 'location']
    for field_name in other_important_fields:
        if field_name in extraction_result.fields:
            field = extraction_result.fields[field_name]
            if field.values:
                values = field.values[:3]  # åªå–å‰3ä¸ªå€¼
                summary["å…³é”®ä¿¡æ¯"][field_display_names.get(field_name, field_name)] = {
                    "å€¼": [v.value for v in values],
                    "ç½®ä¿¡åº¦": f"{field.confidence:.2f}",
                    "æ¥æº": values[0].source
                }
    
    # ä¿å­˜å®Œæ•´æ‘˜è¦
    summary_file = output_path.replace('.json', '_summary.json')
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜ç®€æ´æ‘˜è¦ï¼ˆçº¯æ–‡æœ¬ï¼‰
    text_file = output_path.replace('.json', '_summary.txt')
    with open(text_file, 'w', encoding='utf-8') as f:
        f.write("=" * 50 + "\n")
        f.write("æ ‡ä¹¦å…³é”®ä¿¡æ¯æ‘˜è¦\n")
        f.write("=" * 50 + "\n\n")
        
        # æ–‡æ¡£ä¿¡æ¯
        f.write("ã€æ–‡æ¡£ä¿¡æ¯ã€‘\n")
        doc_info = summary["æ–‡æ¡£ä¿¡æ¯"]
        for key, value in doc_info.items():
            f.write(f"{key}: {value}\n")
        f.write("\n")
        
        # å…³é”®ä¿¡æ¯
        f.write("ã€å…³é”®ä¿¡æ¯ã€‘\n")
        key_info = summary["å…³é”®ä¿¡æ¯"]
        for key, info in key_info.items():
            f.write(f"{key}: {info['å€¼']}\n")
            f.write(f"  ç½®ä¿¡åº¦: {info['ç½®ä¿¡åº¦']}, æ¥æº: {info['æ¥æº']}\n")
            f.write("\n")
    
    return summary_file, text_file


if __name__ == "__main__":
    app() 